{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9016fcb-7a89-4b1d-9a4a-9a098ae453b0",
   "metadata": {},
   "source": [
    "# Prediction of Health Measures Based on Location\n",
    "\n",
    "**Data Scientist:** Elina Rankova\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.licdn.com/dms/image/C4D12AQE3ln6Z4Mo8Pw/article-cover_image-shrink_720_1280/0/1642087174753?e=1718236800&v=beta&t=etvT9WV2qS3g7hAnCds-vQIn3ob9JqJn9vvB9annTWM\">\n",
    "</p>\n",
    "<a href=\"https://www.linkedin.com/pulse/benefits-population-health-management-phm-amazing-charts/\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b1442-8f72-4ee8-80e5-41b16304b79d",
   "metadata": {},
   "source": [
    "## 1. Business Problem and Understanding\n",
    "\n",
    "**Stakeholders:** Company executives specializing in operational expansion to improve public health prevention where it is most needed.\n",
    "\n",
    "As technology evolves, public health and healthcare in particular has long been been lagging behind. In recent years, more companies are leaning on data and technology to inform growth opportunities with the goal of helping those most in need.\n",
    "\n",
    "As an independent contractor specializing in expanding resource availability to undeserved populations, this project aims to identify overall health disparity across the United States utilizing public county and census data provided by the CDC. Proving population health measure specifics based on an overall Health Disparity Index can help direct those striving to close care gaps to expand where there is the greatest need.\n",
    "\n",
    "For Phase 1 of this initiative we will define a Health Disparity Index (HDI) based on a number of health measures and population by geolocation.\n",
    "\n",
    "**The goal:** Provide clients with a holistic understanding of the measures contributing to an overall Health Disparity Index aggregated by geolocation to better understand what United States regions most provide opportunity for growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971f6e0f-f507-4eac-97d0-8cc17c65c622",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     26\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Capstone/src\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics_cv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricsCV\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gp_minimize\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Real, Integer, Categorical\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import numpy as np\n",
    "import random\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedShuffleSplit, cross_validate\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, \\\n",
    "GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.base import clone\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import sys\n",
    "sys.path.append('../Capstone/src')\n",
    "from src.metrics_cv import MetricsCV\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d21bc-27e8-4523-89b1-ee361859421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MetricsCV??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61c73f-b004-43b7-8715-2afa34dc9872",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "\n",
    "For this task we will be using CDC provided PLACES and SDOH county data spanning years 2017-2021\n",
    "Datasets\n",
    "- <a href=\"https://data.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/dv4u-3x3q/about_data\">Local Data for Better Health, County Data 2020 release</a>\n",
    "- <a href=\"https://data.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/pqpp-u99h/about_data\">Local Data for Better Health, County Data 2021 release</a>\n",
    "- <a href=\"https://data.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/duw2-7jbt/about_data\">Local Data for Better Health, County Data 2022 release</a>\n",
    "- <a href=\"https://data.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/swc5-untb/about_data\">Local Data for Better Health, County Data 2023 release</a>\n",
    "- <a href=\"https://data.cdc.gov/500-Cities-Places/SDOH-Measures-for-County-ACS-2017-2021/i6u4-y3g4/about_data\">SDOH Measures for County, ACS 2017-2021</a>\n",
    "\n",
    "Utilizing all the health measures defined within the cagtegories specified below we will create a custom Health Disparity Index (HDI) as our target for this regression task.\n",
    "\n",
    "**Categories within which health measures are defined:** Health Outcomes, Prevention, Health Risk Behaviors, Health Status, Disabilities, and Social Determinants of Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647352e-a3d4-4c2b-9e3b-2fcf6ee758a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "pl1 = pd.read_csv('Data/PLACES_2017-2018.csv')\n",
    "pl2 = pd.read_csv('Data/PLACES_2018-2019.csv')\n",
    "pl3 = pd.read_csv('Data/PLACES_2019-2020.csv')\n",
    "pl4 = pd.read_csv('Data/PLACES_2020-2021.csv')\n",
    "sdoh = pd.read_csv('Data/SDOH_2017-2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5baf7-6c91-41a9-a223-ee5f9f1f2d99",
   "metadata": {},
   "source": [
    "We may want to check on the `StateAbbr` and `StateDesc` since it's showing the country in the preview. We also want to adjust the LocationName to either exclude 'county' or include 'county' to make sure both of the datasets are aligned in nomenclature.\n",
    "\n",
    "Since the data we are working with is by county, we can eliminate the added specification with the `LocationName` in the SDOH dataset.\n",
    "\n",
    "We will also have to deal with the differences in the `Year` column since it is an object and range in the SDOH data and an integer in the PLACES data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d5b2f-ea08-4442-a67a-628a0ef24d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concat all PLACES datasets\n",
    "pl_all = pd.concat([pl1, pl2, pl3, pl4])\n",
    "display(pl_all.tail())\n",
    "\n",
    "sdoh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d95ee-de1d-4a36-90a4-056f6751e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all data to ensure consistency\n",
    "sdoh = sdoh.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "pl_all = pl_all.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Delete the 'county' specification\n",
    "sdoh['LocationName'] = sdoh['LocationName'].str.replace('county',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988456e-0d84-4485-b6a2-7c8cf01a2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sdoh.info())\n",
    "print(pl_all.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442cc8e1-064a-45c1-b894-faf8c0636970",
   "metadata": {},
   "source": [
    "We will have to drop `Low_Confidence_Limit` and `High_Confidence_Limit` since the SDOH dataset does not have these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4c048-aa6f-4b9b-8a29-17e3823978b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sdoh.isna().sum()/sdoh.shape[0],'\\n')\n",
    "print(pl_all.isna().sum()/pl_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a98484-2e14-4ed8-83a1-a504bfe78905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets together\n",
    "df_all = pd.concat([pl_all, sdoh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cba05a-57f6-494c-9463-b7dcea5477e9",
   "metadata": {},
   "source": [
    "Right away we can tell that there are a few columns that are missing too many values and will need to be dropped.\n",
    "- `Data_Value_Footnote_Symbol`\n",
    "- `Data_Value_Footnote`\n",
    "- `Latitude`\n",
    "- `MOE`\n",
    "\n",
    "`Geolocatioin` and `Geolocation` are the same feature when checking the source data websites only the 2020 release has a `Geolocatioin` column while the rest have `Geolocation`. `MeasureID` and `MeasureId` have the same issue it seems.\n",
    "\n",
    "There are object type columns that will need to be transformed. Since state should be treated as a categorical variable we should adjust this in our dataset.\n",
    "\n",
    "In addition, we would also want to make `LocationID` an object since this is not an actual continuous variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b6854-a7c2-4744-ad84-3acf44f69283",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all.info(),'\\n')\n",
    "\n",
    "# Drop unneeded columns\n",
    "df_all.drop(columns=['Data_Value_Footnote_Symbol','Data_Value_Footnote','Latitude',\n",
    "                     'MOE','Low_Confidence_Limit','High_Confidence_Limit'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed27a2-93fe-42b2-9e42-e758dbeba415",
   "metadata": {},
   "source": [
    "We can drop the misspelled `Geolocatioin` after filling in the nas in `Geolocation`. This concept applies to `MeasureID` and `MeasureId` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a846d9-4b91-498d-b9ac-a16765863400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs in Geolocation and with Geolocatioin\n",
    "df_all.loc[df_all['Geolocation'].isna(), 'Geolocation'] = df_all.loc[df_all['Geolocation'].isna(), 'Geolocatioin']\n",
    "\n",
    "# Apply the same method to `MeasureID\n",
    "df_all.loc[df_all['MeasureID'].isna(), 'MeasureID'] = df_all.loc[df_all['MeasureID'].isna(), 'MeasureId']\n",
    "\n",
    "# Drop unneeded columns\n",
    "df_all.drop(columns=['MeasureId','Geolocatioin'],axis=1,inplace=True)\n",
    "\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da92db-2641-4e45-9663-998d3ddcb452",
   "metadata": {},
   "source": [
    "We can drop nas in any columns we are not already dropping such as `LocationName` and the updated `Geolocation`. `LocationID` refers to the zipcode so we should first see if we can impute with the proper zipcode before droping nas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a07f7-2f7b-48e7-931b-0378d66c4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all.isna().sum()/df_all.shape[0])\n",
    "\n",
    "df_all.dropna(subset=['LocationName','Geolocation'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6a7f5-2f30-4110-a79c-9031b06675c1",
   "metadata": {},
   "source": [
    "Looks like we were able to fill the nas using the associated `LocationID` with existing `Geolocation` as a unique identifier for the zipcode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4293102b-4702-4847-83f4-978d53b2a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with non-null LocationID\n",
    "loc_df = df_all.loc[df_all['LocationID'].notna(), ['Geolocation', 'LocationID']]\n",
    "\n",
    "# Create LocationName <> LocationID dictionary\n",
    "loc_dict = dict(zip(loc_df['Geolocation'], loc_df['LocationID']))\n",
    "\n",
    "# Fill missing LocationID values based on LocationName from the dictionary\n",
    "df_all['LocationID'] = df_all['LocationID'].fillna(df_all['Geolocation'].map(loc_dict))\n",
    "\n",
    "# Drop the rest of the nas in LocationID\n",
    "df_all.dropna(inplace=True)\n",
    "\n",
    "# Check the percentage of missing values in df_all after filling missing values\n",
    "print(df_all.isna().sum() / df_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d03308-6072-4429-856f-56297f42ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'StateAbbr' and 'LocationID' columns to appropriate type\n",
    "df_all['StateAbbr'] = df_all['StateAbbr'].astype('category')\n",
    "df_all['LocationID'] = df_all['LocationID'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e804a5-f327-4c64-abbe-a208550b3d34",
   "metadata": {},
   "source": [
    "### Feature Exploration\n",
    "\n",
    "<ins>**Observations**</ins>\n",
    "\n",
    "- We will likely drop `Year` as it can only provide relevant information for the data we retrieved from the PLACES datasets. Otherwise, we have the same date range as seen in the added SDOH data (2017-2021).\n",
    "- Looks like we may want to drop the rows with US as the state as it is an error. We can also drop `StateDesc` since it's the same info as `StateAbbr`.\n",
    "- `DataSource` can be dropped since all of the information is coming from the Behavioral Risk Factor Surveillance System or the 5-year American Community Survey which only contains SDOH data.\n",
    "- `Data_Value_Unit` can be dropped since all of our data values are in percentages.\n",
    "- `Data_Value_Type` will be helpful for interpretation of our predictions since it denotes what percentage the `Data_Value` represents. `DataValueTypeID` corresponds to this feature and can be dropped since `Data_Value_Type` is more informative. \n",
    "- `CategoryID` corresponds to `Category` so we can drop it since `Category` is easier to interpret.\n",
    "- `MeasureId` corresponds the same way to `Measure`. However `Measure` values can be quite lengthy depending on the measure. We also have `Short_Question_Text` corresponding to these features and is more informative than `MeausureId` but shorter than `Measure` so we will keep `Short_Question_Text` and create a reference dictionary before dropping the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048c21b-cd41-47d6-ba53-c601a39f6c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_all['Year'].value_counts(),'\\n')\n",
    "print(df_all['StateAbbr'].value_counts(),'\\n')\n",
    "print(df_all['DataSource'].value_counts(),'\\n')\n",
    "print(df_all['Data_Value_Unit'].value_counts(),'\\n')\n",
    "print(df_all['Data_Value_Type'].value_counts(),'\\n')\n",
    "print(df_all['CategoryID'].value_counts(),'\\n')\n",
    "print(df_all['MeasureID'].value_counts(),'\\n')\n",
    "print(df_all['DataValueTypeID'].value_counts(),'\\n')\n",
    "print(df_all['Short_Question_Text'].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb884c-1fb8-49d7-94ba-3f427cf6ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reference dictionary for category and measure\n",
    "measure_df = df_all[['Measure','Short_Question_Text','MeasureID']]\n",
    "measure_dict = dict(zip(measure_df['MeasureID'], zip(measure_df['Short_Question_Text'], measure_df['Measure'])))\n",
    "category_df = df_all[['Category','CategoryID']]\n",
    "category_dict = dict(zip(category_df['Category'],category_df['CategoryID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04534181-e1aa-47c0-9708-af1d1bd3b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with US as state\n",
    "df_all = df_all.loc[df_all['StateAbbr'] != 'US']\n",
    "\n",
    "# Drop other relevant columns\n",
    "df_all.drop(columns=['DataSource','Data_Value_Unit','CategoryID','DataValueTypeID',\n",
    "                     'StateDesc', 'MeasureID','Year','Measure'],inplace=True)\n",
    "df_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64204a5d-0c36-4286-bc47-970f0739836b",
   "metadata": {},
   "source": [
    "Let's quickly check for duplicates since we had to download all of the PLACES datasets separately to accomodate the year range in the SDOH dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a720ce-5f98-47f9-99da-6f8f779ed65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = df_all.duplicated()\n",
    "dups.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a130c0-7692-4268-bf7b-d962709ab70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identified duplicates\n",
    "df_all.drop_duplicates(ignore_index=True, inplace=True)\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd0605-a58c-4b9e-aac8-db13ad6cf5d7",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "\n",
    "Before we move onto defining the target, we should look at the numerical distribution of the `Data_Value` and `TotalPopulation` columns. \n",
    "\n",
    "We can see there are some outliers in both columns we should consider visualizing and potentially dropping to get a more accurate representation of our distributions.\n",
    "\n",
    "We will want to consider normalizing the `Data_Value` column since the values are represented differently based on type. We can also consider weighing it if we consider one type more important than another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990d848-4151-49f0-9c69-ea31944d3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['Data_Value','TotalPopulation']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edf399-3593-4860-b582-078c8a73f7a4",
   "metadata": {},
   "source": [
    "When visualizing these features, we can estimate that any `Data_Value` above ~85 is an outlier. It is harder to determine for `TotalPopulation`. We should look at the values of these more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5e9ba-b123-4716-9ce3-f05dc6435ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2,figsize=(12,6))\n",
    "sns.boxplot(df_all['Data_Value'],ax=ax[0])\n",
    "ax[0].set_title('Data Value Distribution')\n",
    "\n",
    "sns.boxplot(df_all['TotalPopulation'],ax=ax[1])\n",
    "ax[1].set_title('Total Population Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02775d3-c98c-49af-88f3-d83ba644f6c2",
   "metadata": {},
   "source": [
    "Taking closer at the number of rows that are outside of the the IQR, there are quite a few rows with population and data value outside of bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a5ee9-f19f-4f73-a33a-9eb476609c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_all[['TotalPopulation', 'Data_Value']].columns:\n",
    "    Q1 = df_all[col].quantile(.25)\n",
    "    Q3 = df_all[col].quantile(.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df_all[(df_all[col] < lower_bound) | (df_all[col] > upper_bound)]\n",
    "    print(f'{col} outliers:',outliers.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5bb0f-fda3-4af9-98e1-7085ad6b33fc",
   "metadata": {},
   "source": [
    "Double checking each data type to see if they behave differently in relation to it's own data type vs collectively we see there is indeed a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753b6e1-3f72-49b5-9ba3-d4880c4e5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_outliers = 0\n",
    "outlier_info = []\n",
    "for data, data_type in [('Data_Value', 'Data_Value_Type')]:\n",
    "    # IQR for 'Data_Value' filtered by 'Data_Value_Type'\n",
    "    grouped = df_all.groupby(data_type)\n",
    "    for name, group in grouped:\n",
    "        Q1 = group[data].quantile(0.25)\n",
    "        Q3 = group[data].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Detecting outliers\n",
    "        outliers = group[(group[data] < lower_bound) | (group[data] > upper_bound)]\n",
    "        num_outliers = outliers.shape[0]\n",
    "        total_outliers += num_outliers\n",
    "        outlier_info.append((name, outliers.shape[0]))\n",
    "\n",
    "# Print outlier counts for each type\n",
    "for name, count in outlier_info:\n",
    "    print(f'{name} outliers: {count}')\n",
    "\n",
    "print(f'Total outliers: {total_outliers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3c414-61ba-449e-bc11-93b3ebdda53b",
   "metadata": {},
   "source": [
    "Doing the same with total population, we don't see the same difference so we can treat `TotalPopulation` collectively in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc119421-efad-461d-a229-e4cb274b4812",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_outliers = 0\n",
    "outlier_info = []\n",
    "for data, data_type in [('TotalPopulation', 'Data_Value_Type')]:\n",
    "    # IQR for 'Data_Value' filtered by 'Data_Value_Type'\n",
    "    grouped = df_all.groupby(data_type)\n",
    "    for name, group in grouped:\n",
    "        Q1 = group[data].quantile(0.25)\n",
    "        Q3 = group[data].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Detecting outliers\n",
    "        outliers = group[(group[data] < lower_bound) | (group[data] > upper_bound)]\n",
    "        num_outliers = outliers.shape[0]\n",
    "        total_outliers += num_outliers\n",
    "        outlier_info.append((name, outliers.shape[0]))\n",
    "\n",
    "# Print outlier counts for each type\n",
    "for name, count in outlier_info:\n",
    "    print(f'{name} outliers: {count}')\n",
    "\n",
    "print(f'Total outliers: {total_outliers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb892b24-218d-4181-a5ed-8434816975ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to store indices of outliers\n",
    "outlier_indices = set()  # Set to store indices of outliers\n",
    "\n",
    "# Identify 'TotalPopulation' outliers\n",
    "col = 'TotalPopulation'\n",
    "Q1 = df_all[col].quantile(0.25)\n",
    "Q3 = df_all[col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = df_all[(df_all[col] < lower_bound) | (df_all[col] > upper_bound)]\n",
    "outlier_indices.update(outliers.index)\n",
    "\n",
    "\n",
    "col = 'Data_Value'\n",
    "data_type_col = 'Data_Value_Type'\n",
    "grouped = df_all.groupby(data_type_col)\n",
    "for name, group in grouped:\n",
    "    Q1 = group[col].quantile(0.25)\n",
    "    Q3 = group[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Detecting outliers within each group\n",
    "    outliers = group[(group[col] < lower_bound) | (group[col] > upper_bound)]\n",
    "    outlier_indices.update(outliers.index)\n",
    "\n",
    "df_norm = df_all.drop(index=outlier_indices).copy()\n",
    "\n",
    "\n",
    "print(df_all.shape)\n",
    "print(df_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838464f-a0be-4b4a-96fc-0ba7b622f0e9",
   "metadata": {},
   "source": [
    "It looks like our Data Value distribution for each Data Value Type isn't normal. So before we move on using it to create the HDI we want to create a new column with the normalize values using the `RobustScaler` since we have been using IQR to identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdfb319-8e8f-4ffe-9e1f-da3a2074bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3,figsize=(15,8))\n",
    "\n",
    "sns.kdeplot(df_norm['Data_Value'].loc[df_norm['Data_Value_Type']=='crude prevalence'],ax=ax[0])\n",
    "ax[0].set_title('Crude Prevalance')\n",
    "sns.kdeplot(df_norm['Data_Value'].loc[df_norm['Data_Value_Type']=='age-adjusted prevalence'],ax=ax[1])\n",
    "ax[1].set_title('Age-adjusted Prevalance')\n",
    "sns.kdeplot(df_norm['Data_Value'].loc[df_norm['Data_Value_Type']=='percentage'],ax=ax[2])\n",
    "ax[2].set_title('Percentage')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9135a87d-03b5-409f-a9e3-5399bf08c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Create an empty column for scaled values in the original DataFrame\n",
    "df_norm.loc[:, 'Scaled_Value'] = pd.NA\n",
    "\n",
    "# Process each data type separately\n",
    "for data_type in df_norm['Data_Value_Type'].unique():\n",
    "    # Filter the df to only include rows of the current data type\n",
    "    subset = df_norm[df_norm['Data_Value_Type'] == data_type]\n",
    "    \n",
    "    # Reshape is used because scaler expects a 2D array\n",
    "    scaled_values = scaler.fit_transform(subset['Data_Value'].values.reshape(-1, 1))\n",
    "\n",
    "    # Assign the scaled values back to the appropriate rows in the main DataFrame\n",
    "    df_norm.loc[subset.index, 'Scaled_Value'] = scaled_values.flatten()\n",
    "\n",
    "# Ensure the scaled value is a float\n",
    "df_norm['Scaled_Value'] = df_norm['Scaled_Value'].astype('float')\n",
    "\n",
    "\n",
    "df_norm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eefa25-253d-46cf-8ed6-672796e80f9e",
   "metadata": {},
   "source": [
    "### Defining Health Disparity Index\n",
    "\n",
    "Now that our dataframe is aligned with our project goals, we can define our target variable `Health_Disparity_Index`. \n",
    "\n",
    "To start, we will create a general index taking the sum of all scaled values for each location and create a feature in the full dataset to represent this disparity index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa401d1c-43a4-4b00-bf37-190ea38b3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Scaled_Value per Geoplocation\n",
    "sum_idx = df_norm.groupby('Geolocation')['Scaled_Value'].sum().reset_index()\n",
    "sum_idx.rename({'Scaled_Value': 'Sum_Idx'},axis=1,inplace=True)\n",
    "\n",
    "# Copy dataframe\n",
    "df_sum_idx = df_norm.copy()\n",
    "\n",
    "# Create new column\n",
    "df_sum_idx = df_sum_idx.merge(sum_idx,on='Geolocation')\n",
    "df_sum_idx.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a600b53-6a0f-4db2-ac3d-6a5762e815d2",
   "metadata": {},
   "source": [
    "Comparing random samples of `Sum_Idx` vs `Data_Value` shows us that there is a lot more uniformity in the `Scaled_Value` before aggregation of all of the data value types. This makes sense since our aggregated `Sum_Idx` is combining all values per geolocation, creating and therefore creates less variability when visualizing by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e02456-c1ad-4956-82fb-5733a3b3004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper plotting function\n",
    "\n",
    "def idx_plot(data,y1,y2,y3,y1_name,y2_name,y3_name):\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(15, 12))\n",
    "\n",
    "    # Visualize by State\n",
    "    # Plot by y1\n",
    "    sns.barplot(data=data, hue='StateAbbr', y=y1.sample(n=100000), \n",
    "                palette='Set2', ax=ax[0,0],legend=False)\n",
    "    ax[0,0].set_title(f'{y1_name} by State')\n",
    "    ax[0,0].set_xlabel('State')\n",
    "    \n",
    "    # Plot by y2\n",
    "    sns.barplot(data=data, hue='StateAbbr', y=y2.sample(n=100000), \n",
    "                palette='Set2', ax=ax[0,1],legend=False)\n",
    "    ax[0,1].set_title(f'{y2_name} by State')\n",
    "    ax[0,1].set_xlabel('State') \n",
    "\n",
    "    # Plot by y3\n",
    "    sns.barplot(data=data, hue='StateAbbr', y=y3.sample(n=100000), \n",
    "                palette='Set2', ax=ax[0,2],legend=False)\n",
    "    ax[0,2].set_title(f'{y3_name} by State')\n",
    "    ax[0,2].set_xlabel('State') \n",
    "    \n",
    "    # Visualize by Geolocation\n",
    "    # Plot by y1\n",
    "    sns.scatterplot(data=data.sample(n=1000), x='Geolocation', y=y1, ax=ax[1, 0])\n",
    "    ax[1, 0].set_title(f'{y1_name} by Geolocation')\n",
    "    ax[1, 0].set_xlabel('Geolocation')\n",
    "\n",
    "    ax[1, 0].set_xticks([])\n",
    "    \n",
    "    # Plot by y2\n",
    "    sns.scatterplot(data=data.sample(n=1000), x='Geolocation', y=y2, ax=ax[1, 1])\n",
    "    ax[1, 1].set_title(f'{y2_name} by Geolocation')\n",
    "    ax[1, 1].set_xlabel('Geolocation')\n",
    "\n",
    "    ax[1, 1].set_xticks([])\n",
    "\n",
    "    # Plot by y3\n",
    "    sns.scatterplot(data=data.sample(n=1000), x='Geolocation', y=y3, ax=ax[1, 2])\n",
    "    ax[1, 2].set_title(f'{y3_name} by Geolocation')\n",
    "    ax[1, 2].set_xlabel('Geolocation')\n",
    "\n",
    "    ax[1, 2].set_xticks([])\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show();\n",
    "\n",
    "idx_plot(df_sum_idx,df_sum_idx['Sum_Idx'],df_sum_idx['Scaled_Value'],df_sum_idx['Data_Value'],'Sum Idx','Scaled Value','Data Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b708322-f252-4c0c-8596-f98e50ce1a36",
   "metadata": {},
   "source": [
    "Let's see what our distribution looks like with an 'Avg_Idx' vs using raw sum of scaled values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4977b-e78d-49cb-b16d-1c4ec050da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Scaled_Value per Geoplocation\n",
    "avg_idx = df_norm.groupby('Geolocation')['Scaled_Value'].mean().reset_index()\n",
    "avg_idx.rename({'Scaled_Value': 'Avg_Idx'},axis=1,inplace=True)\n",
    "\n",
    "# Copy dataframe\n",
    "df_avg_idx = df_norm.copy()\n",
    "\n",
    "# Create new column\n",
    "df_avg_idx = df_avg_idx.merge(avg_idx,on='Geolocation')\n",
    "df_avg_idx.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0aa9fa-24d7-45a0-8715-a124b47a9853",
   "metadata": {},
   "source": [
    "When we create an `Avg_Idx` we now see our geolocation distribution but on a different scale and with more goeographical distinction. \n",
    "\n",
    "> This would make sense if for example, this segregation is representing rural vs urban areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84418b0-383b-43d1-98e2-32709648d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_plot(df_avg_idx,df_avg_idx['Avg_Idx'],df_avg_idx['Scaled_Value'],df_avg_idx['Data_Value'],'Avg Idx','Scaled Value','Data Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0320d1-9457-4154-a19e-bc0d162e1b62",
   "metadata": {},
   "source": [
    "Lastly we will add a population weight to each scaled value, this will adjust our results to consider the density of a region when calculating the HDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a9e80-c819-4ee9-b498-ae66b31d30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Scaled_Value per Geoplocation\n",
    "pw_idx = df_norm.groupby('Geolocation')['TotalPopulation'].sum().reset_index()\n",
    "pw_idx.rename({'TotalPopulation': 'GeoPop'},axis=1,inplace=True)\n",
    "\n",
    "# Copy dataframe\n",
    "df_pw_idx = df_norm.copy()\n",
    "\n",
    "# Create new column\n",
    "df_pw_idx = df_pw_idx.merge(pw_idx,on='Geolocation')\n",
    "\n",
    "# Calculate weights\n",
    "df_pw_idx['PopWeight'] = df_pw_idx['TotalPopulation']/df_pw_idx['GeoPop']\n",
    "\n",
    "# Calculate weighted average of health measures\n",
    "weighted_avg = df_pw_idx.groupby('Geolocation').apply(lambda x: (x['Scaled_Value']*x['PopWeight']).sum()).reset_index(name='Weighted_Idx')\n",
    "\n",
    "df_pw_idx = df_pw_idx.merge(weighted_avg,on='Geolocation')\n",
    "\n",
    "df_pw_idx.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e80705-79dc-4c3c-89b7-778c50c44d02",
   "metadata": {},
   "source": [
    "Looks like our weighted average provides more of a spread than the `Avg_Idx, and the same relationship as the `Avg_Idx` which means population definitely has an impact on the HDI and geolocation does as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07cfbf-1ae6-43b0-84ed-53235a651bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_plot(df_pw_idx,df_pw_idx['Weighted_Idx'],df_pw_idx['Scaled_Value'],df_pw_idx['Data_Value'],'Weighted Idx','Scaled Value','Data Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349c26a-4da2-4fe8-bdbc-fdd1fc15b3f1",
   "metadata": {},
   "source": [
    "### Geolocation to HDI Visualization\n",
    "\n",
    "Before we finalize our decision to use the population weighted HDI, let's visualize the index in comparison to population using the `Geolocation`. To do so, we first have to convert the feature to wkt (well-known text) format for use with the geopandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a9dce-3de0-4931-813e-0bb1f66f1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pw_idx.loc[:,'Geolocation']= df_pw_idx['Geolocation'].apply(lambda x: x.upper())\n",
    "df_pw_idx.loc[:,'Geolocation'] = df_pw_idx['Geolocation'].apply(wkt.loads)\n",
    "df_pw_idx.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a4dcb-15b7-4343-81c6-adce651b31ed",
   "metadata": {},
   "source": [
    "Now we can create a GeoDataFrame with `Geolocation` as the geometrical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beadad5-5369-4015-b373-24ccd015557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_pw = gpd.GeoDataFrame(df_pw_idx,geometry='Geolocation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6353781-27fc-479b-9b4c-1f0bac8a7072",
   "metadata": {},
   "source": [
    "Looking at population and the HDI across all of the geolocations we can see that the higher the population, the lower the HDI. In particular we can see that in part of the west coast, Florida, and part of the east coast demonstrate this inverse relationship.\n",
    "\n",
    "> This inverse relationship may make sense when you consider factors such as but not limited to; greater access to care, diversity of services, and public health funding in areas with more population density.\n",
    "\n",
    "This further confirms our decision to use population as a weight for our HDI definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdbafa-c4fe-4a65-8265-9e920557b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(15, 6))\n",
    "\n",
    "# Plotting 'TotalPopulation'\n",
    "gdf_pw.plot(column='TotalPopulation', legend=True, ax=ax[0])\n",
    "ax[0].set_title('Total Population')\n",
    "ax[0].set_xlim(-130, -65)\n",
    "ax[0].set_ylim(25, 50)\n",
    "\n",
    "# Plotting 'Sum_Idx'\n",
    "gdf_pw.plot(column='Weighted_Idx', legend=True, ax=ax[1])\n",
    "ax[1].set_title('Weighted HDI')\n",
    "ax[1].set_xlim(-130, -65)\n",
    "ax[1].set_ylim(25, 50)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9e8a0-0304-4330-ba4e-0e355a2e4237",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "Since our dataset is so large we will train our model on a smaller training set to save computational power and time. We are also going to drop the columns we used to create our `Weighted_Idx`. \n",
    "\n",
    "> In future itterations we would want to consider using `Geolocation` to create location features and explore spatial relationships. However, to keep computational cost relatively low, we will note this as a Phase 2 addition.\n",
    "\n",
    "We are using `StratifiedShuffleSplit` to create subsets of our data while maintaining proportions of the target variable. Running a subset of the data will give us a way to understand model performance while maintaining computational efficiency. Once we have found the best fitting model, we will run it on our extra hold out data we didn't train on, further validating our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0fcd7d-2edc-49d6-984b-9e2aaa0b6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X, y, and split data\n",
    "\n",
    "X = df_pw_idx.drop(columns=['Weighted_Idx','GeoPop','PopWeight','Scaled_Value','Geolocation'])\n",
    "y = df_pw_idx['Weighted_Idx']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,test_size=.2)\n",
    "\n",
    "# Initialize StratifiedShuffleSplit\n",
    "strat_splitter = StratifiedShuffleSplit(n_splits=1,train_size=.3)\n",
    "\n",
    "# Generate indices for the stratified sample\n",
    "for train_index, _ in strat_splitter.split(X_train, y_train):\n",
    "    X_sampled = X.iloc[train_index]\n",
    "    y_sampled = y.iloc[train_index]\n",
    "    \n",
    "# Generate hold-out sets for final model evaluation\n",
    "mask = np.zeros(len(X_train), dtype=bool)\n",
    "mask[train_index] = True\n",
    "X_holdout = X_train.iloc[~mask]\n",
    "y_holdout = y_train.iloc[~mask]\n",
    "\n",
    "# Conduct new train test split for our subset we will using be training\n",
    "X_samp_train, X_samp_test, y_samp_train, y_samp_test = train_test_split(\n",
    "    X_sampled, y_sampled, stratify=y_sampled, test_size=0.2)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42243eaa-b937-40a3-9801-4259d68eeccf",
   "metadata": {},
   "source": [
    "### Base Model and Metrics\n",
    "\n",
    "`StandardScaler` is used on all our numerical data to make sure we are comparing our features on the same scale. And `OneHotEncoder` is used on the rest of the object or categorical categories. \n",
    "\n",
    "Our base model will be a standard `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518092a2-6ab8-47c5-9057-af7b85ef6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Define num and cat subpipes\n",
    "subpipe_num = Pipeline(steps=[\n",
    "    ('ss', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2))])\n",
    "subpipe_cat = Pipeline(steps=[('ohe', OneHotEncoder(sparse_output=False,handle_unknown='ignore'))])\n",
    "\n",
    "# Initiate ColumnTranformer\n",
    "CT = ColumnTransformer(transformers=[('subpipe_num',subpipe_num,[4,5]),\n",
    "                                     ('subpipe_cat',subpipe_cat,[0,1,2,3,6,7])],\n",
    "                       remainder='passthrough')\n",
    "\n",
    "# Initial Pipeline\n",
    "init_pipe = Pipeline(steps=[('ct',CT),\n",
    "                           ('model',DummyRegressor())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38122259-22b8-4704-8225-0896b19d5cab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_pipe.fit(X_samp_train,y_samp_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ea3f6-c0f9-473f-9088-c9cb1a46a079",
   "metadata": {},
   "source": [
    "It looks like our primary RMSE metric is pretty good. However, it is expected that it would be small considering the range of our HDI so we will aim to improve these further. $R^2$ can also stand to improve. Thankfully, we don't see much overfitting or underfitting.\n",
    "\n",
    "We will keep these metrics in mind as we test other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab690ae-ab66-47f3-b275-4d26be06b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Metrics\n",
    "print('Train RSME:', root_mean_squared_error(y_samp_train,init_pipe.predict(X_samp_train)))\n",
    "print('Test RSME:', root_mean_squared_error(y_samp_test,init_pipe.predict(X_samp_test)))\n",
    "print('Train R-Squared:', r2_score(y_samp_train,init_pipe.predict(X_samp_train)))\n",
    "print('Test R-Squared:', r2_score(y_samp_test,init_pipe.predict(X_samp_test)))\n",
    "print('Train MAE:', mean_absolute_error(y_samp_train,init_pipe.predict(X_samp_train)))\n",
    "print('Test MAE:', mean_absolute_error(y_samp_test,init_pipe.predict(X_samp_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f64d59-f995-4c2f-ae92-acf170a6ccd0",
   "metadata": {},
   "source": [
    "Even after dropping a significant amount of features, we are left with fairly high dimentionality. This is something we will want to consider as we test and refine our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483dc6b-3674-4573-9f8a-3dad7f596b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans = init_pipe['ct'].transform(X_samp_train)\n",
    "df_trans = pd.DataFrame(X_train_trans,columns=init_pipe['ct'].get_feature_names_out())\n",
    "print(df_trans.shape)\n",
    "df_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c86579-3a6d-4695-9dca-3ce3a680071c",
   "metadata": {},
   "source": [
    "## 4. Linear Regression Model\n",
    "\n",
    "We will start by testing different regularization techniques to address our moderate $R^2$ and improve our error metrics in this high dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57461e7a-2432-40aa-aa86-777b0570010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create new pipeline\n",
    "\n",
    "def new_pipe(estimator):\n",
    "    return Pipeline(steps=[\n",
    "        ('ct', CT),  \n",
    "        ('model', estimator)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebb950-7a0a-4f92-b4c3-69831a00f0c0",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1560d9-d65c-47c6-962d-f17a7fce5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Replace in pipeline\n",
    "\n",
    "svr_pipe = new_pipe(SVR())\n",
    "svr_pipe.fit(X_samp_train,y_samp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c820f-11a3-4164-8cf9-29720962c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcv_svr = MetricsCV(svr_pipe,'svr',X_samp_train,y_samp_train)\n",
    "\n",
    "params_svr = {'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              'model__C': [0.1, 1, 10, 100, 1000],\n",
    "              'model__degree': [2,3,4,5],\n",
    "              'model__gamma': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "mcv_svr.run_rcv(params=params_svr,scoring=\"neg_root_mean_squared_error\",\n",
    "                                             verbose=3,n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d332f3-e994-48fe-ad61-ed90d2865137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation scores and best estimator\n",
    "val_scores, svr_best = mcv_svr.rcv_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e95027-2597-4aa3-9813-977a4dfd07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = mcv_svr.test_metrics(X_samp_test,y_samp_test)\n",
    "display(val_scores)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e578f-820e-4d17-be93-a042022e7b18",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e652c-bcec-4b59-9bc2-cb97160c6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lasso_pipe = new_pipe(Lasso(max_iter=1000))\n",
    "lasso_pipe.fit(X_samp_train,y_samp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5b5d3-8a09-46a9-9fe9-6c8cb640241a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcv_lasso = MetricsCV(lasso_pipe,'Lasso',X_samp_train,y_samp_train)\n",
    "\n",
    "space_lasso = [Real(10**-4,10**3,'log-uniform',name='alpha'),\n",
    "              Categorical(['cyclic','random'],name='selection')]\n",
    "\n",
    "\n",
    "\n",
    "val_scores, lasso_best = mcv_lasso.objective(space=space_lasso,scoring='neg_root_mean_squared_error',\n",
    "                                           verbose=3, n_calls=15, n_initial_points=5,n_jobs=-1,n_points=4,dx_stopper=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb40ce-d32f-4061-a9d3-ffd5463aff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best params\n",
    "lasso_best['model'].get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23281f11-e694-4fda-9fea-0a8b78137bbe",
   "metadata": {},
   "source": [
    "Looks like our Lasso model did slightly worse in error metrics and significantly worse in $R^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f29153-11e4-476e-890d-d3db577a8cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_scores = mcv_lasso.test_metrics(X_samp_test,y_samp_test)\n",
    "display(val_scores)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c59ebe-9b27-4cbd-b433-dc6743bbb81c",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "\n",
    "Now we can assess the Ridge model in comparison to Lasso to see which type of regularization is best for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c885430-ec04-4341-bd41-4c485bf1a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace in pipeline\n",
    "\n",
    "ridge_pipe = new_pipe(Ridge(random_state=seed_value))\n",
    "ridge_pipe.fit(X_samp_train,y_samp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f787fd-c396-4432-b495-b784b00bc264",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcv_ridge = MetricsCV(ridge_pipe,'Ridge',X_samp_train,y_samp_train)\n",
    "\n",
    "params_ridge = {'model__alpha': [.0001,.001,.01,100,1000],\n",
    "                'model__solver': ['auto','cholesky','lsqr','sag'],\n",
    "                'model__tol': [.0001,.001,.01]\n",
    "}\n",
    "\n",
    "mcv_ridge.run_rcv(params=params_ridge,scoring=\"neg_root_mean_squared_error\",\n",
    "                                             verbose=3,n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207aa0e-0017-4e6d-a2a7-bdd992d8d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation scores and best estimator\n",
    "val_scores, ridge_best = mcv_ridge.rcv_metrics(val_df=val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a06e8-74df-4509-bf08-57251ef2379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best params\n",
    "ridge_best['model'].get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c57b06f-821a-49bc-96d3-b1aab2431be9",
   "metadata": {},
   "source": [
    "Our Ridge model definitely helped lower our error metrics as well as raise $R^2$ vs our Lasso model. However, These scores are about as good as our base metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367f355-7bc7-4d4f-b926-881caa4c638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = mcv_ridge.test_metrics(X_samp_test,y_samp_test,test_df=test_scores)\n",
    "display(val_scores)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835fa50-4ed2-4b2b-bce9-8ba5251b9594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3327681-5607-4977-9e3b-fc0a527d15fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6e2ab-d079-47b5-a356-dc2e82b4dc11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399d840-d93d-4cc2-8244-611fa3b9e4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc724bf-653e-4e17-ae16-f330dc381656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7459c062-31b6-42d3-88dc-f95b88e12b5b",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Since we are not seeing a big enough improvement after testing both Ridge and Lasso regularization, to address the low $R^2$ and lack of representation of the variance in our data we will work on a neural network using the `keras` within the `tensorflow` library.\n",
    "\n",
    "> Due to this environment's incompatibility with the `scikeras`library from which we would download the `KerasRegression` wrapper to insert our neural network into the pipeline, a custom class was created to mimic the function of the wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dee72b-b983-4374-96bb-ba8952696099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, build_fn=None, random_state=None, **kwargs):\n",
    "        self.build_fn = build_fn\n",
    "        self.random_state = random_state\n",
    "        self.kwargs = kwargs  # Store constructor kwargs\n",
    "        self.model_ = None\n",
    "        self.history_ = None  # To store the fit history\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "\n",
    "        # Prepare model_kwargs based on the signature of build_fn\n",
    "        # It filters out fit_params and includes only relevant kwargs for build_fn\n",
    "        sig_params = inspect.signature(self.build_fn).parameters\n",
    "        build_kwargs = {k: v for k, v in self.kwargs.items() if k in sig_params}\n",
    "        build_kwargs.update(fit_params.get('build_kwargs', {}))  # Include any build-specific parameters from fit_params\n",
    "        \n",
    "        # Assuming 'input_dim' must be specified if not already included\n",
    "        if 'input_dim' not in build_kwargs and 'input_shape' not in build_kwargs and X is not None:\n",
    "            build_kwargs['input_dim'] = X.shape[1]\n",
    "\n",
    "        self.model_ = self.build_fn(random_state=self.random_state, **build_kwargs)\n",
    "        self.history_ = self.model_.fit(X, y, **{k: v for k, v in fit_params.items() if k not in ['build_kwargs']})\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model_.predict(X).flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        mse = mean_squared_error(y, predictions)\n",
    "        return -mse  \n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'build_fn': self.build_fn, 'random_state': self.random_state, **self.kwargs}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        # Update kwargs with any set parameters\n",
    "        self.kwargs.update(params)\n",
    "        # Ensure that all parameters are set correctly\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e0918-6456-4ff5-80da-da6ee2892971",
   "metadata": {},
   "source": [
    "Here we will play with the neural network architecture until we get scores providing us with progress in our $R^2$ and error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc4a19-de8c-4171-bcd7-39d3506a559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the Keras model\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "def build_model(optimizer='adam', \n",
    "                units=100, \n",
    "                activation='relu',\n",
    "                input_dim=4425, \n",
    "                random_state=seed_value):\n",
    "    \n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        tf.random.set_seed(random_state)\n",
    "        \n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(units, activation=activation),\n",
    "        Dense(units, activation=activation),\n",
    "        Dense(units, activation=activation),\n",
    "        Dense(units, activation=activation),\n",
    "        Dense(units, activation=activation),\n",
    "        Dense(units, activation=activation),\n",
    "        Dense(1000, activation=activation),\n",
    "        Dropout(0.5),\n",
    "        Dense(units, activation=activation),\n",
    "        Dense(units, activation=activation, kernel_regularizer=l1_l2(l1=.01,l2=.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed036176-afe3-4f5b-85e2-704dc62713bb",
   "metadata": {},
   "source": [
    "Since we have limited CPU power and want to get efficient yet accurate results, we will implement early stopping as well as reduce our learning rate as the difference in our loss function reaches a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f947051-3bc0-4733-852a-71c2171227c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wrap the model with KerasRegressor\n",
    "nn_model = KerasRegressor(build_fn=build_model,random_state=seed_value)\n",
    "\n",
    "# Define baseline error to achieve based off baseline hold out RMSE score\n",
    "baseline_mse = 0.03646619211292927**2 # Calculating the MSE from RMSE\n",
    "\n",
    "# Define early stopping parameters\n",
    "early_stopping = EarlyStopping(monitor='val_loss',min_delta=1e-6, patience=30, verbose=3, \n",
    "                               mode='min', restore_best_weights=True, baseline=baseline_mse)\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, min_lr=0.0001, verbose=3)\n",
    "\n",
    "fit_params = {\n",
    "    'model__epochs': 200,\n",
    "    'model__batch_size': 1000,\n",
    "    'model__verbose': 1,\n",
    "    'model__callbacks': [early_stopping, reduce_lr],\n",
    "    'model__validation_split': 0.2\n",
    "}\n",
    "\n",
    "# Replace in pipeline\n",
    "nn_pipe = new_pipe(KerasRegressor(build_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81e545-d431-40fa-9f3f-5683ec59f9be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model to training data\n",
    "nn_pipe.fit(X_samp_train,y_samp_train, **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d3533-333e-45bb-8fdd-1a56aa594061",
   "metadata": {},
   "source": [
    "Comparing our results not unexpectedly, the Neural Network is overfitting but the train scores are improving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef1fe4-6b86-4be2-99d6-2c74c909f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcv_nn = MetricsCV(nn_pipe,'Neural Network',X_samp_train,y_samp_train)\n",
    "\n",
    "# Get validation and test metrics\n",
    "val_scores = mcv_nn.cross_val(scoring='neg_root_mean_squared_error',val_df=val_scores,verbose=3)\n",
    "test_scores = mcv_nn.test_metrics(X_samp_test,y_samp_test,test_df=test_scores,test_pipe=nn_pipe)\n",
    "\n",
    "display(val_scores)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f04373-4525-4d98-8c36-1c8d30c344d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead849a-a870-4375-9a5f-c1144cf636b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a0cb2b-ccfe-4296-abee-47ac8ec6e73e",
   "metadata": {},
   "source": [
    "## VotingRegressor\n",
    "\n",
    "Let's see if we can get the best of the strongest models so far to get a holistic improvement in metrics. Hopefully this addresses the overfitting seen in the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f9c1d-992f-4880-8792-dd9b0b5345af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted averaging \n",
    "\n",
    "estimators = [\n",
    "    ('ridge',ridge_best['model']),\n",
    "    ('nn',nn_pipe['model'])]\n",
    "\n",
    "voting_pipe = new_pipe(VotingRegressor(estimators=estimators))\n",
    "\n",
    "voting_pipe.fit(X_samp_train,y_samp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca518af-8873-4fd5-b432-09a6b9f1edc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcv_voting = MetricsCV(voting_pipe,'Voting',X_samp_train,y_samp_train)\n",
    "\n",
    "params_voting = {\"model__weights\": [[0.57, 0.43],[0.55, 0.45]]}\n",
    "\n",
    "\n",
    "\n",
    "mcv_voting.run_rcv(params=params_voting,scoring=\"neg_root_mean_squared_error\",\n",
    "                                             verbose=3,n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f9ad5-cb58-4d16-b097-6c6758e85f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation scores and best estimator\n",
    "val_scores, voting_best = mcv_voting.rcv_metrics(val_df=val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1b2cd-f74e-4e47-a156-d6970c048b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best params\n",
    "voting_best['model'].get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0efe46-5a34-47ea-a62d-f71977580b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = mcv_voting.test_metrics(X_samp_test,y_samp_test,test_df=test_scores)\n",
    "display(val_scores)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e191e5-e208-474b-9304-e199188da260",
   "metadata": {},
   "source": [
    "## StackingRegressor\n",
    "\n",
    "Lastly, we will try another ensemble model with the `StackingRegressor`. We are going to include the `VotingRegressor` to see if we can capture more of the patterns through stacking.\n",
    "\n",
    "The idea that the `StackingRegressor` will be able to learn the complex patterns not being captured by voting alone. Since each model in the ensemble might be picking up different patterns, evaluating each separately and learning from the different errors creates a robust new estimator that can address the variety in the data we haven't been able to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87bc35-bc9a-41a8-b7f4-23c0e973978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('ridge',ridge_best['model']),\n",
    "    ('nn',nn_pipe['model']),\n",
    "    ('voting',voting_pipe)]\n",
    "\n",
    "stacking_pipe = new_pipe(StackingRegressor(estimators=estimators))\n",
    "\n",
    "stacking_pipe.fit(X_samp_train,y_samp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56ad4a-2eb1-4e3f-b997-fb316f2a841c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcv_stacking = MetricsCV(stacking_pipe,'Stacking',X_samp_train,y_samp_train)\n",
    "\n",
    "# Get validation and test metrics\n",
    "val_scores = mcv_stacking.cross_val(scoring='neg_root_mean_squared_error',val_df=val_scores,verbose=3)\n",
    "test_scores = mcv_stacking.test_metrics(stacking_pipe,X_samp_test,y_samp_test,test_df=test_scores)\n",
    "\n",
    "display(val_scores)\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689aaa86-194e-4de1-bc43-b86441465e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_scores = mcv_stacking.test_metrics(X_samp_test,y_samp_test,test_df=test_scores)\n",
    "# display(val_scores)\n",
    "# test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548134f6-32ab-4643-b0a0-54335eee4f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Cross validate to verify results\n",
    "\n",
    "# cv_results = cross_validate(stacking_pipe,X_samp_train,y_samp_train, verbose=3,\n",
    "#                             cv=5, scoring=('r2','neg_root_mean_squared_error','neg_mean_absolute_error'),\n",
    "#                             return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2009ce-c74c-4087-896e-4b9496120428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Baseline Metrics\n",
    "# print('Train RSME:', root_mean_squared_error(y_samp_train,stacking_pipe.predict(X_samp_train)))\n",
    "# print('Test RSME:', root_mean_squared_error(y_samp_test,stacking_pipe.predict(X_samp_test)))\n",
    "# print('Train R-Squared:', r2_score(y_samp_train,stacking_pipe.predict(X_samp_train)))\n",
    "# print('Test R-Squared:', r2_score(y_samp_test,stacking_pipe.predict(X_samp_test)))\n",
    "# print('Train MAE:', mean_absolute_error(y_samp_train,stacking_pipe.predict(X_samp_train)))\n",
    "# print('Test MAE:', mean_absolute_error(y_samp_test,stacking_pipe.predict(X_samp_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b3617-46ed-4cb2-9313-e014fd6e63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CV Metrics\n",
    "# print('Val Train RSME:', -cv_results['train_neg_root_mean_squared_error'].mean())\n",
    "# print('Val Test RSME:', -cv_results['test_neg_root_mean_squared_error'].mean())\n",
    "# print('Val Train R-Squared:', cv_results['train_r2'].mean())\n",
    "# print('Val Test R-Squared:', cv_results['test_r2'].mean())\n",
    "# print('Val Train MAE:', -cv_results['train_neg_mean_absolute_error'].mean())\n",
    "# print('Val Test MAE:', -cv_results['test_neg_mean_absolute_error'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96cb75-37ef-40e2-a4d7-8a34f9349d41",
   "metadata": {},
   "source": [
    "## Final Testing\n",
    "\n",
    "To further solidify our findings, let's test the final model on our holdout data and test data set aside during data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e3f6a-573a-4fdb-9721-71e123e8a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcv_final = MetricsCV(stacking_pipe,\"Stacking\",X_holdout,y_holdout)\n",
    "\n",
    "# Final holdout metrics\n",
    "final_test_scores = mcv_final.test_metrics(X_holdout,y_holdout,test_df=final_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6407f7-d37e-4ead-ba60-aefca3eb0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_scores = mcv_final.test_metrics(X_test,y_test,test_df=final_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad3d70-881d-4952-888c-930d296314b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Attempt to serialize and deserialize the pipeline\n",
    "# serialized_pipe = pickle.dumps(voting_pipe)\n",
    "# deserialized_pipe = pickle.loads(serialized_pipe)\n",
    "\n",
    "# # Test on sample data\n",
    "# print(deserialized_pipe.predict(X_samp_train[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e8a2b-9d3c-4710-b7fd-b5452095a709",
   "metadata": {},
   "source": [
    "## Final Evaluation & Conclusion\n",
    "\n",
    "[final evaluation]\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "\n",
    "**Data Limitation and Future Considerations:**\n",
    "\n",
    "Our data set is quite large, while strategies were taken to compensate for the lack of machinery able to efficiently handle this amount of data, in future iterations it would be recommended to invest in more powerful processing to implement more advanced techniques such as `GridSearchCV` that may be more costly.\n",
    "\n",
    "In addition, since we have `Geolocation` available we would want to create location features and explore spatial relationships through methods such as proximity clustering.\n",
    "\n",
    "We also had to drop the year associeated with each record. In the future, we would either want to seek to add a year to SDOH measure records or conduct a sub analysis of the other categories provided in the PLACES datasets.\n",
    "\n",
    "Lastly, there were incompatibilities with the `scikeras` package and the programming environment leading to a need for a custom KerasRegressor. We would want to resolve these issues and perform modeling within a compatible environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2035b-81ce-452e-ab7d-f1d5abb32f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274cb87-6d25-4f37-840f-6c88e04adede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ffbf3-1310-44fd-971b-16d2a6148500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f505330-b8fe-4b84-a9f3-108084220b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236eb34d-a629-49d2-a8dc-1bcb6730cdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hdi_cdc)",
   "language": "python",
   "name": "hdi_cdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
